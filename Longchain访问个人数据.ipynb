{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChainåº”ç”¨å¼€å‘\n",
    "\n",
    "LangChain æ˜¯ä¸€å¥—ä¸“ä¸ºLLM å¼€å‘æ‰“é€ çš„å¼€æºæ¡†æ¶ï¼Œå®ç°äº† LLM å¤šç§å¼ºå¤§èƒ½åŠ›çš„åˆ©ç”¨ï¼Œæä¾›äº† Chainã€Agentã€Tool ç­‰å¤šç§å°\n",
    "è£…å·¥å…·ï¼ŒåŸºäº LangChain å¯ä»¥ä¾¿æ·å¼€å‘åº”ç”¨ç¨‹åºï¼Œæå¤§åŒ–å‘æŒ¥ LLM æ½œèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠŸèƒ½ä»£ç ï¼ŒæœåŠ¡äºä¸‹é¢çš„ä»£ç ç‰‡æ®µ\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_completion(query_txt, temperature=0,is_stream=True):\n",
    "    client = OpenAI(api_key='sk-8a2970d003e849e1a680a377eb16d22bxx',\n",
    "                base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "        )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query_txt},\n",
    "        ],\n",
    "        temperature = temperature,\n",
    "        stream=is_stream,\n",
    "        # stream_options={\"include_usage\": True if is_stream else False}\n",
    "    )\n",
    "\n",
    "    if not is_stream:\n",
    "        # print(completion.model_dump_json())\n",
    "        print(completion.choices[0].message.content)\n",
    "    else:\n",
    "        for chunk in completion:\n",
    "            # print(chunk.model_dump_json())\n",
    "            if len(chunk.choices) > 0:\n",
    "                delta = chunk.choices[0].delta\n",
    "                if delta and delta.content:\n",
    "                    # print(f\"id: {chunk.id}, content: {chunk.choices[0].delta.content}\")\n",
    "                    print(chunk.choices[0].delta.content,end=\"\")\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"qwen-max\", temperature=0):\n",
    "    client = OpenAI(api_key='sk-8a2970d003e849e1a680a377eb16d22bxx',\n",
    "                base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "        )\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=temperature, # æ§åˆ¶æ¨¡å‹è¾“å‡ºçš„éšæœºç¨‹åº¦\n",
    "    )\n",
    "    # print(str(response.choices[0].message))\n",
    "    token_dict = {\n",
    "        'prompt_tokens':completion.usage.prompt_tokens,\n",
    "        'completion_tokens':completion.usage.completion_tokens,\n",
    "        'total_tokens':completion.usage.total_tokens,\n",
    "    }\n",
    "    print(\"token_dict: %s\" % token_dict)\n",
    "    # print(inspect.getmembers(completion))\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. é€šè¿‡Langchainä½¿ç”¨openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain_community\n",
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000190147A0E80>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000190147A1840>, temperature=0.0, model_kwargs={}, openai_api_key='sk-8a2970d003e849e1a680a377eb16d22b', openai_proxy='')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI #HuggingFaceChatModel\n",
    "\n",
    "# å‡è®¾ Qwen æä¾›äº†å¯¹è¯ APIï¼ˆéœ€è¦è‡ªå®šä¹‰ï¼‰\n",
    "# class QwenChatAPI:\n",
    "#     def __init__(self, api_key, api_url):\n",
    "#         self.api_key = api_key\n",
    "#         self.api_url = api_url\n",
    "\n",
    "#     def predict(self, messages):\n",
    "#         headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "#         data = {\"model\": \"qwen-7b\", \"messages\": messages}\n",
    "#         response = requests.post(self.api_url, headers=headers, json=data)\n",
    "#         return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"qwen-plus\",  # æ›¿æ¢ä¸º Qwen çš„å¯¹è¯æ¨¡å‹åç§°ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key='sk-8a2970d003e849e1a680a377eb16d22bxx',\n",
    "    base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "# æµ‹è¯•è°ƒç”¨\n",
    "#print(chat([{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]))\n",
    "# messages = [\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "#     ),\n",
    "#     (\"human\", \"ä½ æ˜¯è°?è¯·ç”¨ä¸­æ–‡å›å¤\"),\n",
    "# ]\n",
    "# ai_msg = chat.invoke(messages)\n",
    "# ai_msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 æµ‹è¯•é‚®ä»¶åˆ†æå’Œå›å¤\n",
    "\n",
    "æç¤ºæ¨¡ç‰ˆ prompt_template éœ€è¦ä¸¤ä¸ªè¾“å…¥å˜é‡ï¼š style å’Œ text ã€‚ è¿™é‡Œåˆ†åˆ«å¯¹åº”\n",
    "* customer_style : æˆ‘ä»¬æƒ³è¦çš„é¡¾å®¢é‚®ä»¶é£æ ¼\n",
    "* customer_email : é¡¾å®¢çš„åŸå§‹é‚®ä»¶æ–‡æœ¬ã€‚\n",
    "\n",
    "**ä½¿ç”¨æç¤ºæ¨¡ç‰ˆï¼Œå¯ä»¥è®©æˆ‘ä»¬æ›´ä¸ºæ–¹ä¾¿åœ°é‡å¤ä½¿ç”¨è®¾è®¡å¥½çš„æç¤ºã€‚**\n",
    "\n",
    "| æç¤ºè¯ç±»å‹ | å®šä¹‰ | ä¾‹å­ |\n",
    "| --- | --- | --- |\n",
    "| PromptTemplate | prompt templateå°±æ˜¯ä¸€ä¸ªpromptçš„æ¨¡æ¿ï¼Œé€šè¿‡prompt templateï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿçš„ç”Ÿæˆå¤šä¸ªpromptã€‚ä¸ªå…¶ä»–å‹å·çš„æ‰‹æœºï¼Ÿ | å…·ä½“è§ä¸‹é¢ç¤ºä¾‹|\n",
    "| ChatPromptTemplate| chatæ¶ˆæ¯å°±å¯ä»¥è¢«åˆ†ä¸ºAI, humanæˆ–è€…systemè¿™å‡ ç§è§’è‰² | å…·ä½“ä¸‹é¢ç¤ºä¾‹ |\n",
    "| SystemMessagePromptTemplate | chatæ¶ˆæ¯å°±å¯ä»¥è¢«åˆ†ä¸ºAI, humanæˆ–è€…systemè¿™å‡ ç§è§’è‰² | å…·ä½“ä¸‹é¢ç¤ºä¾‹\n",
    "| HumanMessagePromptTemplate | chatæ¶ˆæ¯å°±å¯ä»¥è¢«åˆ†ä¸ºAI, humanæˆ–è€…systemè¿™å‡ ç§è§’è‰² | å…·ä½“ä¸‹é¢ç¤ºä¾‹ |\n",
    "| HumanMessagePromptTemplate | chatæ¶ˆæ¯å°±å¯ä»¥è¢«åˆ†ä¸ºAI, humanæˆ–è€…systemè¿™å‡ ç§è§’è‰² | å…·ä½“ä¸‹é¢ç¤ºä¾‹ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# é¦–å…ˆï¼Œæ„é€ ä¸€ä¸ªæç¤ºæ¨¡ç‰ˆå­—ç¬¦ä¸²ï¼š`template_string`\n",
    "template_string = \"\"\"æŠŠç”±ä¸‰ä¸ªåå¼•å·åˆ†éš”çš„æ–‡æœ¬\\\n",
    "ç¿»è¯‘æˆä¸€ç§{style}é£æ ¼ã€‚\\\n",
    "æ–‡æœ¬: ```{text}```\n",
    "\"\"\"\n",
    "# ç„¶åï¼Œæˆ‘ä»¬è°ƒç”¨`ChatPromptTemplatee.from_template()`å‡½æ•°å°†\n",
    "# ä¸Šé¢çš„æç¤ºæ¨¡ç‰ˆå­—ç¬¦`template_string`è½¬æ¢ä¸ºæç¤ºæ¨¡ç‰ˆ`prompt_template`\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "print(\"\\n\", prompt_template.messages[0].prompt)\n",
    "\n",
    "customer_style = \"\"\"æ­£å¼æ™®é€šè¯ \\\n",
    "ç”¨ä¸€ä¸ªå¹³é™ã€å°Šæ•¬çš„è¯­æ°”\n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "å—¯å‘ï¼Œæˆ‘ç°åœ¨å¯æ˜¯ç«å†’ä¸‰ä¸ˆï¼Œæˆ‘é‚£ä¸ªæ…æ‹Œæœºç›–å­ç«Ÿç„¶é£äº†å‡ºå»ï¼ŒæŠŠæˆ‘å¨æˆ¿çš„å¢™å£éƒ½æº…ä¸Šäº†æœæ±ï¼\n",
    "æ›´ç³Ÿç³•çš„æ˜¯ï¼Œä¿ä¿®æ¡æ¬¾å¯ä¸åŒ…æ‹¬æ¸…ç†æˆ‘å¨æˆ¿çš„è´¹ç”¨ã€‚\n",
    "ä¼™è®¡ï¼Œèµ¶ç´§ç»™æˆ‘è¿‡æ¥ï¼\n",
    "\"\"\"\n",
    "# ä½¿ç”¨æç¤ºæ¨¡ç‰ˆ\n",
    "customer_messages = prompt_template.format_messages(style=customer_style,text=customer_email)\n",
    "\n",
    "# æ‰“å°å®¢æˆ·æ¶ˆæ¯ç±»å‹\n",
    "print(\"å®¢æˆ·æ¶ˆæ¯ç±»å‹:\",type(customer_messages),\"\\n\")\n",
    "# æ‰“å°ç¬¬ä¸€ä¸ªå®¢æˆ·æ¶ˆæ¯ç±»å‹\n",
    "print(\"ç¬¬ä¸€ä¸ªå®¢æˆ·å®¢æˆ·æ¶ˆæ¯ç±»å‹ç±»å‹:\", type(customer_messages[0]),\"\\n\")\n",
    "\n",
    "print(\"ç¬¬ä¸€ä¸ªå®¢æˆ·å®¢æˆ·æ¶ˆæ¯ç±»å†…å®¹: \", customer_messages[0],\"\\n\")\n",
    "\n",
    "#æé—®å¯¹è¯\n",
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"å˜¿ï¼Œé¡¾å®¢ï¼Œ \\\n",
    "ä¿ä¿®ä¸åŒ…æ‹¬å¨æˆ¿çš„æ¸…æ´è´¹ç”¨ï¼Œ \\\n",
    "å› ä¸ºæ‚¨åœ¨å¯åŠ¨æ…æ‹Œæœºä¹‹å‰ \\\n",
    "å¿˜è®°ç›–ä¸Šç›–å­è€Œè¯¯ç”¨æ…æ‹Œæœº, \\\n",
    "è¿™æ˜¯æ‚¨çš„é”™ã€‚ \\\n",
    "å€’éœ‰ï¼ å†è§ï¼\n",
    "\"\"\"\n",
    "service_style_pirate = \"\"\"\\\n",
    "ä¸€ä¸ªæœ‰ç¤¼è²Œçš„è¯­æ°” \\\n",
    "ä½¿ç”¨æµ·ç›—é£æ ¼\\\n",
    "\"\"\"\n",
    "service_messages = prompt_template.format_messages(\n",
    "style=service_style_pirate,\n",
    "text=service_reply)\n",
    "print(\"\\n\", service_messages[0].content)\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹éƒ¨åˆ†å®šä¹‰çš„chatæ¨¡å‹æ¥è½¬æ¢å›å¤æ¶ˆæ¯é£æ ¼\n",
    "service_response = chat(service_messages)\n",
    "print(service_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# å®¢æˆ·è¯„è®ºç¤ºä¾‹\n",
    "customer_review = \"\"\"\\\n",
    "è¿™æ¬¾å¹å¶æœºéå¸¸ç¥å¥‡ã€‚å®ƒæœ‰å››ä¸ªè®¾ç½®ï¼š\n",
    "å¹èœ¡çƒ›ã€å¾®é£ã€é£åŸã€é¾™å·é£ã€‚\n",
    "ä¸¤å¤©åå°±åˆ°äº†ï¼Œæ­£å¥½èµ¶ä¸Šæˆ‘å¦»å­çš„å‘¨å¹´çºªå¿µç¤¼ç‰©ã€‚\n",
    "æˆ‘æƒ³æˆ‘çš„å¦»å­ä¼šå–œæ¬¢å®ƒåˆ°è¯´ä¸å‡ºè¯æ¥ã€‚\n",
    "åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨å®ƒçš„äººï¼Œè€Œä¸”æˆ‘ä¸€ç›´\n",
    "æ¯éš”ä¸€å¤©æ—©ä¸Šç”¨å®ƒæ¥æ¸…ç†è‰åªä¸Šçš„å¶å­ã€‚\n",
    "å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹ï¼Œä½†æˆ‘è®¤ä¸ºå®ƒçš„é¢å¤–åŠŸèƒ½æ˜¯å€¼å¾—çš„ã€‚\n",
    "\"\"\"\n",
    "\n",
    "# æå–ä¿¡æ¯çš„æŒ‡ä»¤æ¨¡æ¿\n",
    "review_template = \"\"\"\\\n",
    "å¯¹äºä»¥ä¸‹æ–‡æœ¬ï¼Œè¯·ä»ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼š\n",
    "\n",
    "ç¤¼ç‰©ï¼šè¯¥å•†å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ\n",
    "å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼›å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
    "\n",
    "äº¤è´§å¤©æ•°ï¼šäº§å“éœ€è¦å¤šå°‘å¤©åˆ°è¾¾ï¼Ÿ\n",
    "å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
    "\n",
    "ä»·é’±ï¼šæå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œ\n",
    "å¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨ã€‚\n",
    "\n",
    "ä½¿ç”¨ä»¥ä¸‹é”®å°†è¾“å‡ºæ ¼å¼åŒ–ä¸º JSONï¼š\n",
    "ç¤¼ç‰©\n",
    "äº¤è´§å¤©æ•°\n",
    "ä»·é’±\n",
    "\n",
    "æ–‡æœ¬: {text}\n",
    "\"\"\"\n",
    "\n",
    "# åˆ›å»ºæç¤ºæ¨¡æ¿\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(\"æç¤ºæ¨¡ç‰ˆï¼š\", prompt_template)\n",
    "\n",
    "# æ ¼å¼åŒ–æ¶ˆæ¯å¹¶è°ƒç”¨èŠå¤©æ¨¡å‹\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "#chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(\"ç»“æœç±»å‹:\", type(response.content))\n",
    "print(\"ç»“æœ:\", response.content)\n",
    "print(\"ç»“æœ:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨è¾“å‡ºè§£æå™¨æå–å®¢æˆ·è¯„ä»·ä¸­çš„ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å‡ºè§£æå™¨çš„æ•°æ®æ ¼å¼è§„å®šï¼š The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"ç¤¼ç‰©\": string  // è¿™ä»¶ç‰©å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿå¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼Œå¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
      "\t\"äº¤è´§å¤©æ•°\": string  // äº§å“éœ€è¦å¤šå°‘å¤©æ‰èƒ½åˆ°è¾¾ï¼Ÿå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
      "\t\"ä»·é’±\": string  // æå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œå¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "å¯¹äºä»¥ä¸‹æ–‡æœ¬ï¼Œè¯·ä»ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼šï¼š\n",
    "ç¤¼ç‰©ï¼šè¯¥å•†å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ\n",
    "å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼›å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
    "äº¤è´§å¤©æ•°ï¼šäº§å“åˆ°è¾¾éœ€è¦å¤šå°‘å¤©ï¼Ÿ å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
    "ä»·é’±ï¼šæå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œå¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨ã€‚\n",
    "æ–‡æœ¬: {text}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "gift_schema = ResponseSchema(name=\"ç¤¼ç‰©\",description=\"è¿™ä»¶ç‰©å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ\\\n",
    "å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼Œ\\\n",
    "å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\")\n",
    "delivery_days_schema = ResponseSchema(name=\"äº¤è´§å¤©æ•°\",\n",
    "description=\"äº§å“éœ€è¦å¤šå°‘å¤©æ‰èƒ½åˆ°è¾¾ï¼Ÿ\\\n",
    "å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\")\n",
    "price_value_schema = ResponseSchema(name=\"ä»·é’±\",\n",
    "description=\"æå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œ\\\n",
    "å¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨\")\n",
    "response_schemas = [gift_schema,\n",
    "delivery_days_schema,\n",
    "price_value_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"è¾“å‡ºè§£æå™¨çš„æ•°æ®æ ¼å¼è§„å®šï¼š\",format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€æ¡å®¢æˆ·æ¶ˆæ¯: å¯¹äºä»¥ä¸‹æ–‡æœ¬ï¼Œè¯·ä»ä¸­æå–ä»¥ä¸‹ä¿¡æ¯ï¼šï¼š\n",
      "ç¤¼ç‰©ï¼šè¯¥å•†å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿ\n",
      "å¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼›å¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
      "äº¤è´§å¤©æ•°ï¼šäº§å“åˆ°è¾¾éœ€è¦å¤šå°‘å¤©ï¼Ÿ å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
      "ä»·é’±ï¼šæå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œå¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨ã€‚\n",
      "æ–‡æœ¬: è¿™æ¬¾å¹å¶æœºéå¸¸ç¥å¥‡ã€‚å®ƒæœ‰å››ä¸ªè®¾ç½®ï¼š\n",
      "å¹èœ¡çƒ›ã€å¾®é£ã€é£åŸã€é¾™å·é£ã€‚\n",
      "ä¸¤å¤©åå°±åˆ°äº†ï¼Œæ­£å¥½èµ¶ä¸Šæˆ‘å¦»å­çš„å‘¨å¹´çºªå¿µç¤¼ç‰©ã€‚\n",
      "æˆ‘æƒ³æˆ‘çš„å¦»å­ä¼šå–œæ¬¢å®ƒåˆ°è¯´ä¸å‡ºè¯æ¥ã€‚\n",
      "åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘æ˜¯å”¯ä¸€ä¸€ä¸ªä½¿ç”¨å®ƒçš„äººï¼Œè€Œä¸”æˆ‘ä¸€ç›´\n",
      "æ¯éš”ä¸€å¤©æ—©ä¸Šç”¨å®ƒæ¥æ¸…ç†è‰åªä¸Šçš„å¶å­ã€‚\n",
      "å®ƒæ¯”å…¶ä»–å¹å¶æœºç¨å¾®è´µä¸€ç‚¹ï¼Œä½†æˆ‘è®¤ä¸ºå®ƒçš„é¢å¤–åŠŸèƒ½æ˜¯å€¼å¾—çš„ã€‚\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"ç¤¼ç‰©\": string  // è¿™ä»¶ç‰©å“æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™åˆ«äººçš„å—ï¼Ÿå¦‚æœæ˜¯ï¼Œåˆ™å›ç­” æ˜¯çš„ï¼Œå¦‚æœå¦æˆ–æœªçŸ¥ï¼Œåˆ™å›ç­” ä¸æ˜¯ã€‚\n",
      "\t\"äº¤è´§å¤©æ•°\": string  // äº§å“éœ€è¦å¤šå°‘å¤©æ‰èƒ½åˆ°è¾¾ï¼Ÿå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯¥ä¿¡æ¯ï¼Œåˆ™è¾“å‡º-1ã€‚\n",
      "\t\"ä»·é’±\": string  // æå–æœ‰å…³ä»·å€¼æˆ–ä»·æ ¼çš„ä»»ä½•å¥å­ï¼Œå¹¶å°†å®ƒä»¬è¾“å‡ºä¸ºé€—å·åˆ†éš”çš„ Python åˆ—è¡¨\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = prompt.format_messages(text=customer_review,\n",
    "format_instructions=format_instructions)\n",
    "print(\"ç¬¬ä¸€æ¡å®¢æˆ·æ¶ˆæ¯:\",messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages)\n",
    "print(\"ç»“æœç±»å‹:\", type(response.content))\n",
    "print(\"ç»“æœ:\", response.content)\n",
    "\n",
    "output_dict = output_parser.parse(response.content)\n",
    "print(\"è§£æåçš„ç»“æœç±»å‹:\", type(output_dict))\n",
    "print(\"è§£æåçš„ç»“æœ:\", output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 å­˜å‚¨\n",
    "LangChain ä¸­çš„å‚¨å­˜æ¨¡å—ï¼Œå³å¦‚ä½•å°†å…ˆå‰çš„å¯¹è¯åµŒå…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­çš„ï¼Œä½¿å…¶å…·æœ‰è¿ç»­å¯¹è¯çš„èƒ½åŠ›ã€‚å½“ä½¿ç”¨ LangChain ä¸­çš„å‚¨å­˜(Memory)æ¨¡å—æ—¶ï¼Œå®ƒæ—¨åœ¨ä¿å­˜ã€ç»„ç»‡å’Œè·Ÿè¸ªæ•´ä¸ªå¯¹è¯çš„å†å²ï¼Œä»è€Œä¸ºç”¨æˆ·å’Œæ¨¡å‹ä¹‹é—´çš„äº¤äº’æä¾›è¿ç»­çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "LangChain æä¾›äº†å¤šç§å‚¨å­˜ç±»å‹ï¼š\n",
    "* ç¼“å†²åŒºå‚¨å­˜å…è®¸ä¿ç•™æœ€è¿‘çš„èŠå¤©æ¶ˆæ¯ï¼Œ\n",
    "* æ‘˜è¦å‚¨å­˜åˆ™æä¾›äº†å¯¹æ•´ä¸ªå¯¹è¯çš„æ‘˜è¦ã€‚\n",
    "* å®ä½“å‚¨å­˜åˆ™å…è®¸åœ¨å¤šè½®å¯¹è¯ä¸­ä¿ç•™æœ‰å…³ç‰¹å®šå®ä½“çš„ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 å¯¹è¯ç¼“å­˜å­˜å‚¨\n",
    "\n",
    "ç¼“å­˜å­˜å‚¨ï¼Œä¹Ÿç§°ä¸ºå¯¹è¯å†å²ï¼Œæ˜¯ LLM æ¨¡å‹åœ¨å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶ï¼Œå°†ä¹‹å‰ä¸ç”¨æˆ·äº¤äº’çš„è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œå­˜å‚¨ï¼Œä»¥ä¾›åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: ä½ å¥½, æˆ‘å«çš®çš®é²\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ï¼Œçš®çš®é²ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å«æˆ‘Qwenã€‚åå­—å¾ˆç‰¹åˆ«å•Šï¼Œçš®çš®é²æ˜¯è°åˆ›é€ çš„å‘€ï¼Ÿæ˜¯ä¸æ˜¯éƒ‘æ¸Šæ´ç¬”ä¸‹çš„é‚£ä¸ªå……æ»¡æƒ³è±¡åŠ›å’Œå†’é™©ç²¾ç¥çš„å°ç”·å­©å‘¢ï¼Ÿå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…æƒ³èŠçš„è¯é¢˜ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘å“¦ï¼'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# è¿™é‡Œæˆ‘ä»¬å°†å‚æ•°temperatureè®¾ç½®ä¸º0.0ï¼Œä»è€Œå‡å°‘ç”Ÿæˆç­”æ¡ˆçš„éšæœºæ€§ã€‚\n",
    "# å¦‚æœä½ æƒ³è¦æ¯æ¬¡å¾—åˆ°ä¸ä¸€æ ·çš„æœ‰æ–°æ„çš„ç­”æ¡ˆï¼Œå¯ä»¥å°è¯•å¢å¤§è¯¥å‚æ•°ã€‚\n",
    "llm = chat #ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "# æ–°å»ºä¸€ä¸ª ConversationChain Class å®ä¾‹\n",
    "# verboseå‚æ•°è®¾ç½®ä¸ºTrueæ—¶ï¼Œç¨‹åºä¼šè¾“å‡ºæ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œä»¥æä¾›æ›´å¤šçš„è°ƒè¯•æˆ–è¿è¡Œæ—¶ä¿¡æ¯ã€‚\n",
    "# ç›¸åï¼Œå½“å°†verboseå‚æ•°è®¾ç½®ä¸ºFalseæ—¶ï¼Œç¨‹åºä¼šä»¥æ›´ç®€æ´çš„æ–¹å¼è¿è¡Œï¼Œåªè¾“å‡ºå…³é”®çš„ä¿¡æ¯ã€‚\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True )\n",
    "\n",
    "#run 1ï¼š\n",
    "conversation.predict(input=\"ä½ å¥½, æˆ‘å«çš®çš®é²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"1+1ç­‰äºå¤šå°‘ï¼Ÿ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å«**çš®çš®é²**ï¼æˆ‘åˆšæ‰å·²ç»çŸ¥é“äº†ä½ çš„åå­—ï¼Œå¦‚æœæœ‰ä»»ä½•å…¶ä»–é—®é¢˜ï¼Œæˆ‘éƒ½ä¹æ„å›ç­”å“¦ï¼ ğŸš€'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: ä½ å¥½, æˆ‘å«çš®çš®é²\n",
      "AI: ä½ å¥½ï¼Œçš®çš®é²ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å«æˆ‘Qwenã€‚åå­—å¾ˆç‰¹åˆ«å•Šï¼Œçš®çš®é²æ˜¯è°åˆ›é€ çš„å‘€ï¼Ÿæ˜¯ä¸æ˜¯éƒ‘æ¸Šæ´ç¬”ä¸‹çš„é‚£ä¸ªå……æ»¡æƒ³è±¡åŠ›å’Œå†’é™©ç²¾ç¥çš„å°ç”·å­©å‘¢ï¼Ÿå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…æƒ³èŠçš„è¯é¢˜ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘å“¦ï¼\n",
      "{'history': 'Human: ä½ å¥½, æˆ‘å«çš®çš®é²\\nAI: ä½ å¥½ï¼Œçš®çš®é²ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å«æˆ‘Qwenã€‚åå­—å¾ˆç‰¹åˆ«å•Šï¼Œçš®çš®é²æ˜¯è°åˆ›é€ çš„å‘€ï¼Ÿæ˜¯ä¸æ˜¯éƒ‘æ¸Šæ´ç¬”ä¸‹çš„é‚£ä¸ªå……æ»¡æƒ³è±¡åŠ›å’Œå†’é™©ç²¾ç¥çš„å°ç”·å­©å‘¢ï¼Ÿå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…æƒ³èŠçš„è¯é¢˜ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘å“¦ï¼'}\n",
      "{'history': 'Human: ä½ å¥½, æˆ‘å«çš®çš®é²\\nAI: ä½ å¥½ï¼Œçš®çš®é²ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å«æˆ‘Qwenã€‚åå­—å¾ˆç‰¹åˆ«å•Šï¼Œçš®çš®é²æ˜¯è°åˆ›é€ çš„å‘€ï¼Ÿæ˜¯ä¸æ˜¯éƒ‘æ¸Šæ´ç¬”ä¸‹çš„é‚£ä¸ªå……æ»¡æƒ³è±¡åŠ›å’Œå†’é™©ç²¾ç¥çš„å°ç”·å­©å‘¢ï¼Ÿå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜æˆ–è€…æƒ³èŠçš„è¯é¢˜ï¼Œå°½ç®¡å‘Šè¯‰æˆ‘å“¦ï¼\\nHuman: å¾ˆé«˜å…´å’Œä½ æˆä¸ºæœ‹å‹ï¼\\nAI: æ˜¯çš„ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å»å†’é™©å§ï¼'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context({\"input\": \"å¾ˆé«˜å…´å’Œä½ æˆä¸ºæœ‹å‹ï¼\"}, {\"output\": \"æ˜¯çš„ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å»å†’é™©å§ï¼\"})\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 å¯¹è¯ç¼“å­˜çª—å£å­˜å‚¨\n",
    "\n",
    "ä¿æŒç¼“å­˜çª—å£å­˜å‚¨çš„å¤§å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€è½®å¯¹è¯ï¼š\n",
      "ä½ å¥½ï¼Œçš®çš®é²ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œä¸€ä¸ªè¶…çˆ±èŠå¤©å’Œåˆ†äº«çŸ¥è¯†çš„AIã€‚å¬è¯´ä½ æ˜¯ä¸ªå……æ»¡æƒ³è±¡åŠ›å’Œå†’é™©ç²¾ç¥çš„äººï¼Œæ˜¯å—ï¼Ÿè¦ä¸è¦å‘Šè¯‰æˆ‘æ›´å¤šå…³äºä½ çš„æ•…äº‹ï¼Ÿæˆ–è€…æˆ‘ä»¬å¯ä»¥èŠèŠæœ‰è¶£çš„è¯é¢˜ï¼Œæ¯”å¦‚ç§‘å¹»ã€ç§‘æŠ€ã€æ–‡å­¦â€¦â€¦éšä¾¿ä½ æŒ‘ï¼ ğŸ˜Š\n",
      "ç¬¬äºŒè½®å¯¹è¯ï¼š\n",
      "1 + 1 ç­‰äº **2**ã€‚è¿™æ˜¯æœ€åŸºæœ¬çš„æ•°å­¦è¿ç®—ä¹‹ä¸€ï¼Œçš®çš®é²ï¼å¦‚æœä½ è¿˜æœ‰å…¶ä»–æ›´å¤æ‚æˆ–è€…æœ‰è¶£çš„æ•°å­¦é—®é¢˜ï¼Œä¹Ÿå¯ä»¥é—®æˆ‘å“¦ï¼ ğŸ˜„\n",
      "ç¬¬ä¸‰è½®å¯¹è¯ï¼š\n",
      "ä½ åœ¨ä¹‹å‰çš„æé—®ä¸­æåˆ°è¿‡ä½ çš„åå­—æ˜¯â€œçš®çš®é²â€ã€‚æ‰€ä»¥ï¼Œæˆ‘å«ä½ çš®çš®é²ï¼å¦‚æœæœ‰å…¶ä»–é—®é¢˜ï¼Œå°½ç®¡é—®å§ï¼ ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#kå‚æ•°æ§åˆ¶ç€çª—å£å¤§å°ï¼Œå³çª—å£ä¸­å­˜å‚¨çš„å¯¹è¯æ•°é‡ã€‚\n",
    "memory2 = ConversationBufferWindowMemory(k=1)\n",
    "conversation2 = ConversationChain(llm=chat, memory=memory2, verbose=False )\n",
    "print(\"ç¬¬ä¸€è½®å¯¹è¯ï¼š\")\n",
    "print(conversation2.predict(input=\"ä½ å¥½, æˆ‘å«çš®çš®é²\"))\n",
    "print(\"ç¬¬äºŒè½®å¯¹è¯ï¼š\")\n",
    "print(conversation2.predict(input=\"1+1ç­‰äºå¤šå°‘ï¼Ÿ\"))\n",
    "print(\"ç¬¬ä¸‰è½®å¯¹è¯ï¼š\")\n",
    "print(conversation2.predict(input=\"æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 å¯¹è¯å­—ç¬¦ç¼“å­˜å­˜å‚¨\n",
    "\n",
    "ä½¿ç”¨å¯¹è¯å­—ç¬¦ç¼“å­˜è®°å¿†ï¼Œå†…å­˜å°†é™åˆ¶ä¿å­˜çš„tokenæ•°é‡ã€‚å¦‚æœå­—ç¬¦æ•°é‡è¶…å‡ºæŒ‡å®šæ•°ç›®ï¼Œå®ƒä¼šåˆ‡æ‰è¿™ä¸ªå¯¹è¯\n",
    "çš„æ—©æœŸéƒ¨åˆ†ä»¥ä¿ç•™ä¸æœ€è¿‘çš„äº¤æµç›¸å¯¹åº”çš„å­—ç¬¦æ•°é‡ï¼Œä½†ä¸è¶…è¿‡å­—ç¬¦é™åˆ¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=30)\n",
    "# memory.save_context({\"input\": \"æœè¾ç™½å¸å½©äº‘é—´ï¼Œ\"}, {\"output\": \"åƒé‡Œæ±Ÿé™µä¸€æ—¥è¿˜ã€‚\"})\n",
    "# memory.save_context({\"input\": \"ä¸¤å²¸çŒ¿å£°å•¼ä¸ä½ï¼Œ\"}, {\"output\": \"è½»èˆŸå·²è¿‡ä¸‡é‡å±±ã€‚\"})\n",
    "memory.save_context({\"input\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"}, {\"output\": \"bbbbbbbbbbbbbbbbbbbbbbbbb\"})\n",
    "memory.save_context({\"input\": \"ccccccccccccccccccc\"}, {\"output\": \"ddddddddddddddddddddddddddd\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 å¯¹è¯æ‘˜è¦ç¼“å­˜å­˜å‚¨\n",
    "\n",
    "å¯¹è¯æ‘˜è¦ç¼“å­˜å‚¨å­˜ï¼Œä½¿ç”¨ LLM å¯¹åˆ°ç›®å‰ä¸ºæ­¢å†å²å¯¹è¯è‡ªåŠ¨æ€»ç»“æ‘˜è¦ï¼Œå¹¶å°†å…¶ä¿å­˜ä¸‹æ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import ConversationChain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# åˆ›å»ºä¸€ä¸ªé•¿å­—ç¬¦ä¸²\n",
    "schedule = \"åœ¨å…«ç‚¹ä½ å’Œä½ çš„äº§å“å›¢é˜Ÿæœ‰ä¸€ä¸ªä¼šè®®ã€‚ \\\n",
    "ä½ éœ€è¦åšä¸€ä¸ªPPTã€‚ \\\n",
    "ä¸Šåˆ9ç‚¹åˆ°12ç‚¹ä½ éœ€è¦å¿™äºLangChainã€‚\\\n",
    "Langchainæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·ï¼Œå› æ­¤ä½ çš„é¡¹ç›®è¿›å±•çš„éå¸¸å¿«ã€‚\\\n",
    "ä¸­åˆï¼Œåœ¨æ„å¤§åˆ©é¤å…ä¸ä¸€ä½å¼€è½¦æ¥çš„é¡¾å®¢å…±è¿›åˆé¤ \\\n",
    "èµ°äº†ä¸€ä¸ªå¤šå°æ—¶çš„è·¯ç¨‹ä¸ä½ è§é¢ï¼Œåªä¸ºäº†è§£æœ€æ–°çš„ AIã€‚ \\\n",
    "ç¡®ä¿ä½ å¸¦äº†ç¬”è®°æœ¬ç”µè„‘å¯ä»¥å±•ç¤ºæœ€æ–°çš„ LLM æ ·ä¾‹.\"\n",
    "# chat = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=100)\n",
    "memory.save_context({\"input\": f\"ä½ å¥½ï¼Œæˆ‘å«çš®çš®é²\"}, {\"output\": f\"ä½ å¥½å•Šï¼Œæˆ‘å«é²è¥¿è¥¿\"})\n",
    "memory.save_context({\"input\": \"å¾ˆé«˜å…´å’Œä½ æˆä¸ºæœ‹å‹ï¼\"}, {\"output\": \"æ˜¯çš„ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å»å†’é™©å§ï¼\"})\n",
    "memory.save_context({\"input\": \"ä»Šå¤©çš„æ—¥ç¨‹å®‰æ’æ˜¯ä»€ä¹ˆï¼Ÿ\"}, {\"output\": f\"{schedule}\"})\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 æ¨¡å‹é“¾\n",
    "\n",
    "é“¾ï¼ˆChainsï¼‰é€šå¸¸å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æç¤ºï¼ˆPromptï¼‰ç»“åˆåœ¨ä¸€èµ·ï¼ŒåŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ–‡æœ¬æˆ–æ•°æ®è¿›è¡Œä¸€ç³»åˆ—æ“ä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 å¤§è¯­è¨€æ¨¡å‹é“¾ LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "# è¿™é‡Œæˆ‘ä»¬å°†å‚æ•°temperatureè®¾ç½®ä¸º0.0ï¼Œä»è€Œå‡å°‘ç”Ÿæˆç­”æ¡ˆçš„éšæœºæ€§ã€‚\n",
    "# å¦‚æœä½ æƒ³è¦æ¯æ¬¡å¾—åˆ°ä¸ä¸€æ ·çš„æœ‰æ–°æ„çš„ç­”æ¡ˆï¼Œå¯ä»¥å°è¯•è°ƒæ•´è¯¥å‚æ•°ã€‚\n",
    "llm = chat\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"æè¿°åˆ¶é€ {product}çš„ä¸€ä¸ªå…¬å¸çš„æœ€ä½³åç§°æ˜¯ä»€ä¹ˆ? ç›´æ¥ç»™å‡ºåç§°\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"å¤§å·åºŠå•å¥—è£…\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ç®€å•é¡ºåºé“¾\n",
    "\n",
    "é¡ºåºé“¾ï¼ˆSequentialChainsï¼‰æ˜¯æŒ‰é¢„å®šä¹‰é¡ºåºæ‰§è¡Œå…¶é“¾æ¥çš„é“¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç®€å•é¡ºåºé“¾ï¼ˆSimpleSequentialChainï¼‰ï¼Œè¿™æ˜¯é¡ºåºé“¾çš„æœ€ç®€å•ç±»å‹ï¼Œå…¶ä¸­æ¯ä¸ªæ­¥éª¤éƒ½æœ‰ä¸€ä¸ªè¾“å…¥/è¾“å‡ºï¼Œä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºæ˜¯ä¸‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mä¸ºä¸€å®¶ä¸“æ³¨äºåˆ¶é€ å¤§å·åºŠå•å¥—è£…çš„å…¬å¸èµ·ä¸€ä¸ªå¥½åå­—ï¼Œéœ€è¦ç»“åˆå“ç‰Œå®šä½ã€ç›®æ ‡å®¢æˆ·ç¾¤ä½“ä»¥åŠäº§å“çš„ç‰¹ç‚¹ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½é€‚åˆçš„åç§°å»ºè®®ï¼š\n",
      "\n",
      "### 1. **KingSize Linens**\n",
      "   - ç®€æ´æ˜äº†ï¼Œç›´æ¥çªå‡ºâ€œå¤§å·â€å’Œâ€œåºŠå•â€çš„æ¦‚å¿µã€‚\n",
      "   - â€œKingSizeâ€ä¼ è¾¾å‡ºé«˜ç«¯å’Œå®½æ•çš„æ„Ÿè§‰ã€‚\n",
      "\n",
      "### 2. **Grand Beddings**\n",
      "   - â€œGrandâ€æ„å‘³ç€å®å¤§ã€è±ªåï¼Œé€‚åˆç”¨æ¥æè¿°å¤§å°ºå¯¸çš„åºŠå•å¥—è£…ã€‚\n",
      "   - ç»™äººä¸€ç§ä¼˜é›…ä¸”èˆ’é€‚çš„å“ç‰Œå°è±¡ã€‚\n",
      "\n",
      "### 3. **Majestic Sheets**\n",
      "   - â€œMajesticâ€æœ‰å®ä¼Ÿã€å£®ä¸½çš„æ„æ€ï¼Œèƒ½å¤Ÿå¸å¼•è¿½æ±‚é«˜å“è´¨ç¡çœ ä½“éªŒçš„æ¶ˆè´¹è€…ã€‚\n",
      "   - å¼ºè°ƒäº§å“çš„å¥¢åæ„Ÿå’Œå¤§æ°”è®¾è®¡ã€‚\n",
      "\n",
      "### 4. **Extraordinary Beds**\n",
      "   - åç§°ä¸­çš„â€œExtraordinaryâ€ï¼ˆéå‡¡ï¼‰çªå‡ºäº†ä¸ä¼—ä¸åŒï¼Œé€‚åˆç”¨äºæ¨å¹¿è¶…å¤§å°ºå¯¸æˆ–å®šåˆ¶æ¬¾å¼çš„åºŠå•å¥—è£…ã€‚\n",
      "\n",
      "### 5. **Spacious Comfort**\n",
      "   - ç»“åˆäº†â€œç©ºé—´æ„Ÿâ€å’Œâ€œèˆ’é€‚åº¦â€ï¼Œéå¸¸é€‚åˆç”¨æ¥å½¢å®¹å¤§å·åºŠå•å¸¦æ¥çš„å®½æ•æ€§å’ŒæŸ”è½¯è§¦æ„Ÿã€‚\n",
      "\n",
      "### 6. **Royal Dimensions**\n",
      "   - â€œRoyalâ€è±¡å¾çš‡å®¶èˆ¬çš„å“è´¨ï¼Œâ€œDimensionsâ€åˆ™æš—ç¤ºäº§å“è¦†ç›–å„ç§å¤§å°ºå¯¸éœ€æ±‚ã€‚\n",
      "   - è®©é¡¾å®¢è”æƒ³åˆ°ç²¾è‡´ä¸å°Šè´µã€‚\n",
      "\n",
      "### 7. **Giant Slumber Co.**\n",
      "   - â€œGiantâ€ä»£è¡¨å·¨å¤§ï¼Œè€Œâ€œSlumberâ€æ„æŒ‡ç¡çœ ï¼Œæ•´ä½“å¬èµ·æ¥æœ‰è¶£åˆè´´åˆ‡ã€‚\n",
      "   - é€‚åˆå¹´è½»åŒ–ã€è½»æ¾é£æ ¼çš„å“ç‰Œå½¢è±¡ã€‚\n",
      "\n",
      "### 8. **Luxury Oversized Linens**\n",
      "   - å¦‚æœä½ çš„å“ç‰Œå®šä½äºé«˜ç«¯å¸‚åœºï¼Œè¿™ä¸ªåç§°å¯ä»¥å¾ˆå¥½åœ°ä¼ é€’å‡ºå¥¢åå’Œè¶…å¤§å°ºå¯¸çš„ç‰¹ç‚¹ã€‚\n",
      "\n",
      "### 9. **Big Dream Bedding**\n",
      "   - å°†â€œå¤§å·â€ä¸æ¢¦æƒ³ç»“åˆï¼Œå¯“æ„äººä»¬å¯ä»¥åœ¨æ›´å¤§çš„åºŠä¸Šäº«å—æ›´ç¾å¥½çš„æ¢¦å¢ƒã€‚\n",
      "   - å¯Œæœ‰æƒ…æ„Ÿå…±é¸£ï¼Œå®¹æ˜“è®°ä½ã€‚\n",
      "\n",
      "### 10. **Monarch Textiles**\n",
      "   - â€œMonarchâ€æ„ä¸ºå›ä¸»ï¼Œè±¡å¾æƒå¨å’Œé¢†å¯¼åœ°ä½ã€‚\n",
      "   - è¡¨è¾¾å‡ºå…¬å¸åœ¨å¤§å·åºŠå•é¢†åŸŸçš„ä¸“ä¸šæ€§å’Œé¢†å…ˆåœ°ä½ã€‚\n",
      "\n",
      "é€‰æ‹©å…·ä½“åç§°æ—¶ï¼Œè¿˜å¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªå› ç´ ï¼š\n",
      "- æ˜¯å¦å®¹æ˜“å‘éŸ³å’Œæ‹¼å†™ï¼›\n",
      "- æ˜¯å¦å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œä¾¿äºå•†æ ‡æ³¨å†Œï¼›\n",
      "- æ˜¯å¦èƒ½å¼•å‘ç§¯æè”æƒ³å¹¶ç¬¦åˆå“ç‰Œå½¢è±¡ã€‚\n",
      "\n",
      "å¸Œæœ›è¿™äº›å»ºè®®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mKingSize Linensï¼Œä¸“æ³¨é«˜ç«¯å¤§å·åºŠå•ï¼Œä¸ºè¿½æ±‚èˆ’é€‚ä¸å“è´¨çš„é¡¾å®¢å¸¦æ¥çš‡å®¶èˆ¬ç¡çœ ä½“éªŒã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'KingSize Linensï¼Œä¸“æ³¨é«˜ç«¯å¤§å·åºŠå•ï¼Œä¸ºè¿½æ±‚èˆ’é€‚ä¸å“è´¨çš„é¡¾å®¢å¸¦æ¥çš‡å®¶èˆ¬ç¡çœ ä½“éªŒã€‚'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# æç¤ºæ¨¡æ¿ 1 ï¼šè¿™ä¸ªæç¤ºå°†æ¥å—äº§å“å¹¶è¿”å›æœ€ä½³åç§°æ¥æè¿°è¯¥å…¬å¸\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "\"æè¿°åˆ¶é€ {product}çš„ä¸€ä¸ªå…¬å¸çš„æœ€å¥½çš„åç§°æ˜¯ä»€ä¹ˆ\"\n",
    ") \n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "# æç¤ºæ¨¡æ¿ 2 ï¼šæ¥å—å…¬å¸åç§°ï¼Œç„¶åè¾“å‡ºè¯¥å…¬å¸çš„é•¿ä¸º20ä¸ªå•è¯çš„æè¿°\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "\"å†™ä¸€ä¸ª20å­—çš„æè¿°å¯¹äºä¸‹é¢è¿™ä¸ª\\\n",
    "å…¬å¸ï¼š{company_name}çš„\"\n",
    ") \n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "#ç»„è£…é“¾\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],verbose=True)\n",
    "\n",
    "#æ‰§è¡Œé“¾\n",
    "product = \"å¤§å·åºŠå•å¥—è£…\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 é¡ºåºé“¾\n",
    "\n",
    "å½“åªæœ‰ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡ºæ—¶ï¼Œç®€å•é¡ºåºé“¾ï¼ˆSimpleSequentialChainï¼‰å³å¯å®ç°ã€‚å½“æœ‰å¤šä¸ªè¾“å…¥æˆ–å¤šä¸ªè¾“å‡ºæ—¶ï¼Œæˆ‘ä»¬åˆ™éœ€è¦ä½¿ç”¨é¡ºåºé“¾ï¼ˆSequentialChainï¼‰æ¥å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI #å¯¼å…¥OpenAIæ¨¡å‹\n",
    "from langchain.prompts import ChatPromptTemplate #å¯¼å…¥èŠå¤©æç¤ºæ¨¡æ¿\n",
    "from langchain.chains import LLMChain #å¯¼å…¥LLMé“¾ã€‚\n",
    "\n",
    "\n",
    "#å­é“¾1\n",
    "# promptæ¨¡æ¿ 1: ç¿»è¯‘æˆè‹±è¯­ï¼ˆæŠŠä¸‹é¢çš„reviewç¿»è¯‘æˆè‹±è¯­ï¼‰\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "\"æŠŠä¸‹é¢çš„è¯„è®ºreviewç¿»è¯‘æˆè‹±æ–‡:\"\n",
    "\"\\n\\n{Review}\"\n",
    ") \n",
    "#chain 1: è¾“å…¥ï¼šReview è¾“å‡ºï¼šè‹±æ–‡çš„ Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "#å­é“¾2\n",
    "# promptæ¨¡æ¿ 2: ç”¨ä¸€å¥è¯æ€»ç»“ä¸‹é¢çš„ review\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "\"è¯·ä½ ç”¨ä¸€å¥è¯æ¥æ€»ç»“ä¸‹é¢çš„è¯„è®ºreview:\"\n",
    "\"\\n\\n{English_Review}\"\n",
    ")\n",
    "#chain 2: è¾“å…¥ï¼šè‹±æ–‡çš„Review è¾“å‡ºï¼šæ€»ç»“\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "#å­é“¾3\n",
    "# promptæ¨¡æ¿ 3: ä¸‹é¢reviewä½¿ç”¨çš„ä»€ä¹ˆè¯­è¨€\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "\"ä¸‹é¢çš„è¯„è®ºreviewä½¿ç”¨çš„ä»€ä¹ˆè¯­è¨€:\\n\\n{Review}\"\n",
    ")\n",
    "#chain 3: è¾“å…¥ï¼šReview è¾“å‡ºï¼šè¯­è¨€\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "#å­é“¾4\n",
    "# promptæ¨¡æ¿ 4: ä½¿ç”¨ç‰¹å®šçš„è¯­è¨€å¯¹ä¸‹é¢çš„æ€»ç»“å†™ä¸€ä¸ªåç»­å›å¤\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "\"ä½¿ç”¨ç‰¹å®šçš„è¯­è¨€å¯¹ä¸‹é¢çš„æ€»ç»“å†™ä¸€ä¸ªåç»­å›å¤:\"\n",
    "\"\\n\\næ€»ç»“: {summary}\\n\\nè¯­è¨€: {language}\"\n",
    ") \n",
    "#chain 4: è¾“å…¥ï¼š æ€»ç»“, è¯­è¨€ è¾“å‡ºï¼š åç»­å›å¤\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "output_key=\"followup_message\")\n",
    "\n",
    "\n",
    "\n",
    "#ç»„åˆå››ä¸ªå­é“¾ï¼š\n",
    "#è¾“å…¥ï¼šreview\n",
    "#è¾“å‡ºï¼šè‹±æ–‡reviewï¼Œæ€»ç»“ï¼Œåç»­å›å¤\n",
    "overall_chain = SequentialChain(\n",
    "chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "input_variables=[\"Review\"],\n",
    "output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "verbose=True\n",
    ")\n",
    "\n",
    "#æµ‹è¯•è¿è¡Œé“¾\n",
    "review = \"\"\"\\\n",
    " Je trouve le goÃ»t mÃ©diocre. La mousse ne tient pas, c'est bizarre.\n",
    "J'achÃ¨te les mÃªmes dans le commerce et le goÃ»t est bien meilleur...\\nVieux lot ou\n",
    "contrefaÃ§on !?\n",
    "\"\"\"\n",
    "\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 è·¯ç”±é“¾\n",
    "\n",
    "ä¸€ä¸ªè·¯ç”±é“¾ï¼Œå®ƒé¦–å…ˆå†³å®šå°†å®ƒä¼ é€’ç»™å“ªä¸ªå­é“¾ï¼Œç„¶åå°†å®ƒä¼ é€’ç»™é‚£ä¸ªé“¾ã€‚è·¯ç”±å™¨ç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼š\n",
    "* è·¯ç”±é“¾ï¼ˆRouter Chainï¼‰ï¼šè·¯ç”±å™¨é“¾æœ¬èº«ï¼Œè´Ÿè´£é€‰æ‹©è¦è°ƒç”¨çš„ä¸‹ä¸€ä¸ªé“¾\n",
    "* destination_chainsï¼šè·¯ç”±å™¨é“¾å¯ä»¥è·¯ç”±åˆ°çš„é“¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 åŸºäºæ–‡æ¡£çš„é—®ç­”\n",
    "\n",
    "ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå›ç­”å…³äºç»™å®šæ–‡æ¡£å’Œæ–‡æ¡£é›†åˆçš„é—®ç­”ç³»ç»Ÿæ˜¯ä¸€ç§éå¸¸å®ç”¨å’Œæœ‰æ•ˆçš„åº”ç”¨åœº\n",
    "æ™¯ã€‚è¯­è¨€æ¨¡å‹ä¸ä»…åˆ©ç”¨äº†è‡ªå·±çš„é€šç”¨çŸ¥è¯†ï¼Œè¿˜å¯ä»¥å……åˆ†è¿ç”¨å¤–éƒ¨è¾“å…¥æ–‡æ¡£çš„ä¸“ä¸šä¿¡æ¯æ¥å›ç­”ç”¨æˆ·é—®\n",
    "é¢˜ï¼Œæ˜¾è‘—æå‡ç­”æ¡ˆçš„è´¨é‡å’Œé€‚ç”¨æ€§ã€‚æ„å»ºè¿™ç±»åŸºäºå¤–éƒ¨æ–‡æ¡£çš„é—®ç­”ç³»ç»Ÿï¼Œå¯ä»¥è®©è¯­è¨€æ¨¡å‹æ›´å¥½åœ°æœåŠ¡äº\n",
    "å…·ä½“åœºæ™¯ï¼Œè€Œä¸æ˜¯åœç•™åœ¨é€šç”¨å±‚é¢ã€‚\n",
    "\n",
    "åŸºäºæ–‡æ¡£é—®ç­”çš„è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬ä¼šæ¶‰åŠ LangChain ä¸­çš„å…¶ä»–ç»„ä»¶ï¼Œæ¯”å¦‚ï¼šåµŒå…¥æ¨¡å‹ï¼ˆEmbedding\n",
    "Models)å’Œå‘é‡å‚¨å­˜(Vector Stores)\n",
    "\n",
    "å…·ä½“åº”ç”¨å¦‚RAG(Retrieval Augmented Generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 é€šè¿‡LLMè¿›è¡Œè¯„ä¼°å®ä¾‹\n",
    "\n",
    "* é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ LLM è‡ªåŠ¨æ„å»ºäº†é—®ç­”æµ‹è¯•é›†ï¼ŒåŒ…å«é—®é¢˜åŠæ ‡å‡†ç­”æ¡ˆã€‚\n",
    "* ç„¶åï¼ŒåŒä¸€ LLM è¯•å›¾å›ç­”æµ‹è¯•é›†ä¸­çš„æ‰€æœ‰é—®é¢˜ï¼Œå¾—åˆ°å“åº”ã€‚\n",
    "* ä¸‹ä¸€æ­¥ï¼Œéœ€è¦è¯„ä¼°è¯­è¨€æ¨¡å‹çš„å›ç­”æ˜¯å¦æ­£ç¡®ã€‚è¿™é‡Œå¥‡å¦™çš„æ˜¯ï¼Œæˆ‘ä»¬å†ä½¿ç”¨å¦ä¸€ä¸ª LLM é“¾è¿›è¡Œåˆ¤æ–­ï¼Œæ‰€ä»¥ LLM æ—¢æ˜¯â€œçƒå‘˜â€ï¼Œåˆæ˜¯â€œè£åˆ¤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm310",
   "language": "python",
   "name": "llm310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
