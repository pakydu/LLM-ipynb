{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain应用开发\n",
    "\n",
    "LangChain 是一套专为LLM 开发打造的开源框架，实现了 LLM 多种强大能力的利用，提供了 Chain、Agent、Tool 等多种封\n",
    "装工具，基于 LangChain 可以便捷开发应用程序，极大化发挥 LLM 潜能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 功能代码，服务于下面的代码片段\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_completion(query_txt, temperature=0,is_stream=True):\n",
    "    client = OpenAI(api_key='sk-8a2970d003e849e1a680a377eb16d22bxx',\n",
    "                base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "        )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-plus\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query_txt},\n",
    "        ],\n",
    "        temperature = temperature,\n",
    "        stream=is_stream,\n",
    "        # stream_options={\"include_usage\": True if is_stream else False}\n",
    "    )\n",
    "\n",
    "    if not is_stream:\n",
    "        # print(completion.model_dump_json())\n",
    "        print(completion.choices[0].message.content)\n",
    "    else:\n",
    "        for chunk in completion:\n",
    "            # print(chunk.model_dump_json())\n",
    "            if len(chunk.choices) > 0:\n",
    "                delta = chunk.choices[0].delta\n",
    "                if delta and delta.content:\n",
    "                    # print(f\"id: {chunk.id}, content: {chunk.choices[0].delta.content}\")\n",
    "                    print(chunk.choices[0].delta.content,end=\"\")\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"qwen-max\", temperature=0):\n",
    "    client = OpenAI(api_key='sk-8a2970d003e849e1a680a377eb16d22bxx',\n",
    "                base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "        )\n",
    "    completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=temperature, # 控制模型输出的随机程度\n",
    "    )\n",
    "    # print(str(response.choices[0].message))\n",
    "    token_dict = {\n",
    "        'prompt_tokens':completion.usage.prompt_tokens,\n",
    "        'completion_tokens':completion.usage.completion_tokens,\n",
    "        'total_tokens':completion.usage.total_tokens,\n",
    "    }\n",
    "    print(\"token_dict: %s\" % token_dict)\n",
    "    # print(inspect.getmembers(completion))\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 通过Langchain使用openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install langchain_community\n",
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000190147A0E80>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000190147A1840>, temperature=0.0, model_kwargs={}, openai_api_key='sk-8a2970d003e849e1a680a377eb16d22b', openai_proxy='')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI #HuggingFaceChatModel\n",
    "\n",
    "# 假设 Qwen 提供了对话 API（需要自定义）\n",
    "# class QwenChatAPI:\n",
    "#     def __init__(self, api_key, api_url):\n",
    "#         self.api_key = api_key\n",
    "#         self.api_url = api_url\n",
    "\n",
    "#     def predict(self, messages):\n",
    "#         headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "#         data = {\"model\": \"qwen-7b\", \"messages\": messages}\n",
    "#         response = requests.post(self.api_url, headers=headers, json=data)\n",
    "#         return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"qwen-plus\",  # 替换为 Qwen 的对话模型名称（如果有）\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key='sk-8a2970d003e849e1a680a377eb16d22bxx',\n",
    "    base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "# 测试调用\n",
    "#print(chat([{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]))\n",
    "# messages = [\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "#     ),\n",
    "#     (\"human\", \"你是谁?请用中文回复\"),\n",
    "# ]\n",
    "# ai_msg = chat.invoke(messages)\n",
    "# ai_msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 测试邮件分析和回复\n",
    "\n",
    "提示模版 prompt_template 需要两个输入变量： style 和 text 。 这里分别对应\n",
    "* customer_style : 我们想要的顾客邮件风格\n",
    "* customer_email : 顾客的原始邮件文本。\n",
    "\n",
    "**使用提示模版，可以让我们更为方便地重复使用设计好的提示。**\n",
    "\n",
    "| 提示词类型 | 定义 | 例子 |\n",
    "| --- | --- | --- |\n",
    "| PromptTemplate | prompt template就是一个prompt的模板，通过prompt template，我们可以快速的生成多个prompt。个其他型号的手机？ | 具体见下面示例|\n",
    "| ChatPromptTemplate| chat消息就可以被分为AI, human或者system这几种角色 | 具体下面示例 |\n",
    "| SystemMessagePromptTemplate | chat消息就可以被分为AI, human或者system这几种角色 | 具体下面示例\n",
    "| HumanMessagePromptTemplate | chat消息就可以被分为AI, human或者system这几种角色 | 具体下面示例 |\n",
    "| HumanMessagePromptTemplate | chat消息就可以被分为AI, human或者system这几种角色 | 具体下面示例 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# 首先，构造一个提示模版字符串：`template_string`\n",
    "template_string = \"\"\"把由三个反引号分隔的文本\\\n",
    "翻译成一种{style}风格。\\\n",
    "文本: ```{text}```\n",
    "\"\"\"\n",
    "# 然后，我们调用`ChatPromptTemplatee.from_template()`函数将\n",
    "# 上面的提示模版字符`template_string`转换为提示模版`prompt_template`\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "print(\"\\n\", prompt_template.messages[0].prompt)\n",
    "\n",
    "customer_style = \"\"\"正式普通话 \\\n",
    "用一个平静、尊敬的语气\n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "嗯呐，我现在可是火冒三丈，我那个搅拌机盖子竟然飞了出去，把我厨房的墙壁都溅上了果汁！\n",
    "更糟糕的是，保修条款可不包括清理我厨房的费用。\n",
    "伙计，赶紧给我过来！\n",
    "\"\"\"\n",
    "# 使用提示模版\n",
    "customer_messages = prompt_template.format_messages(style=customer_style,text=customer_email)\n",
    "\n",
    "# 打印客户消息类型\n",
    "print(\"客户消息类型:\",type(customer_messages),\"\\n\")\n",
    "# 打印第一个客户消息类型\n",
    "print(\"第一个客户客户消息类型类型:\", type(customer_messages[0]),\"\\n\")\n",
    "\n",
    "print(\"第一个客户客户消息类内容: \", customer_messages[0],\"\\n\")\n",
    "\n",
    "#提问对话\n",
    "customer_response = chat(customer_messages)\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"嘿，顾客， \\\n",
    "保修不包括厨房的清洁费用， \\\n",
    "因为您在启动搅拌机之前 \\\n",
    "忘记盖上盖子而误用搅拌机, \\\n",
    "这是您的错。 \\\n",
    "倒霉！ 再见！\n",
    "\"\"\"\n",
    "service_style_pirate = \"\"\"\\\n",
    "一个有礼貌的语气 \\\n",
    "使用海盗风格\\\n",
    "\"\"\"\n",
    "service_messages = prompt_template.format_messages(\n",
    "style=service_style_pirate,\n",
    "text=service_reply)\n",
    "print(\"\\n\", service_messages[0].content)\n",
    "\n",
    "# 调用模型部分定义的chat模型来转换回复消息风格\n",
    "service_response = chat(service_messages)\n",
    "print(service_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 客户评论示例\n",
    "customer_review = \"\"\"\\\n",
    "这款吹叶机非常神奇。它有四个设置：\n",
    "吹蜡烛、微风、风城、龙卷风。\n",
    "两天后就到了，正好赶上我妻子的周年纪念礼物。\n",
    "我想我的妻子会喜欢它到说不出话来。\n",
    "到目前为止，我是唯一一个使用它的人，而且我一直\n",
    "每隔一天早上用它来清理草坪上的叶子。\n",
    "它比其他吹叶机稍微贵一点，但我认为它的额外功能是值得的。\n",
    "\"\"\"\n",
    "\n",
    "# 提取信息的指令模板\n",
    "review_template = \"\"\"\\\n",
    "对于以下文本，请从中提取以下信息：\n",
    "\n",
    "礼物：该商品是作为礼物送给别人的吗？\n",
    "如果是，则回答 是的；如果否或未知，则回答 不是。\n",
    "\n",
    "交货天数：产品需要多少天到达？\n",
    "如果没有找到该信息，则输出-1。\n",
    "\n",
    "价钱：提取有关价值或价格的任何句子，\n",
    "并将它们输出为逗号分隔的 Python 列表。\n",
    "\n",
    "使用以下键将输出格式化为 JSON：\n",
    "礼物\n",
    "交货天数\n",
    "价钱\n",
    "\n",
    "文本: {text}\n",
    "\"\"\"\n",
    "\n",
    "# 创建提示模板\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(\"提示模版：\", prompt_template)\n",
    "\n",
    "# 格式化消息并调用聊天模型\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "#chat = ChatOpenAI(temperature=0.0)\n",
    "response = chat(messages)\n",
    "\n",
    "# 输出结果\n",
    "print(\"结果类型:\", type(response.content))\n",
    "print(\"结果:\", response.content)\n",
    "print(\"结果:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用输出解析器提取客户评价中的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出解析器的数据格式规定： The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"礼物\": string  // 这件物品是作为礼物送给别人的吗？如果是，则回答 是的，如果否或未知，则回答 不是。\n",
      "\t\"交货天数\": string  // 产品需要多少天才能到达？如果没有找到该信息，则输出-1。\n",
      "\t\"价钱\": string  // 提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "对于以下文本，请从中提取以下信息：：\n",
    "礼物：该商品是作为礼物送给别人的吗？\n",
    "如果是，则回答 是的；如果否或未知，则回答 不是。\n",
    "交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。\n",
    "价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表。\n",
    "文本: {text}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "gift_schema = ResponseSchema(name=\"礼物\",description=\"这件物品是作为礼物送给别人的吗？\\\n",
    "如果是，则回答 是的，\\\n",
    "如果否或未知，则回答 不是。\")\n",
    "delivery_days_schema = ResponseSchema(name=\"交货天数\",\n",
    "description=\"产品需要多少天才能到达？\\\n",
    "如果没有找到该信息，则输出-1。\")\n",
    "price_value_schema = ResponseSchema(name=\"价钱\",\n",
    "description=\"提取有关价值或价格的任何句子，\\\n",
    "并将它们输出为逗号分隔的 Python 列表\")\n",
    "response_schemas = [gift_schema,\n",
    "delivery_days_schema,\n",
    "price_value_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(\"输出解析器的数据格式规定：\",format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条客户消息: 对于以下文本，请从中提取以下信息：：\n",
      "礼物：该商品是作为礼物送给别人的吗？\n",
      "如果是，则回答 是的；如果否或未知，则回答 不是。\n",
      "交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。\n",
      "价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表。\n",
      "文本: 这款吹叶机非常神奇。它有四个设置：\n",
      "吹蜡烛、微风、风城、龙卷风。\n",
      "两天后就到了，正好赶上我妻子的周年纪念礼物。\n",
      "我想我的妻子会喜欢它到说不出话来。\n",
      "到目前为止，我是唯一一个使用它的人，而且我一直\n",
      "每隔一天早上用它来清理草坪上的叶子。\n",
      "它比其他吹叶机稍微贵一点，但我认为它的额外功能是值得的。\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"礼物\": string  // 这件物品是作为礼物送给别人的吗？如果是，则回答 是的，如果否或未知，则回答 不是。\n",
      "\t\"交货天数\": string  // 产品需要多少天才能到达？如果没有找到该信息，则输出-1。\n",
      "\t\"价钱\": string  // 提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = prompt.format_messages(text=customer_review,\n",
    "format_instructions=format_instructions)\n",
    "print(\"第一条客户消息:\",messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages)\n",
    "print(\"结果类型:\", type(response.content))\n",
    "print(\"结果:\", response.content)\n",
    "\n",
    "output_dict = output_parser.parse(response.content)\n",
    "print(\"解析后的结果类型:\", type(output_dict))\n",
    "print(\"解析后的结果:\", output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 存储\n",
    "LangChain 中的储存模块，即如何将先前的对话嵌入到语言模型中的，使其具有连续对话的能力。当使用 LangChain 中的储存(Memory)模块时，它旨在保存、组织和跟踪整个对话的历史，从而为用户和模型之间的交互提供连续的上下文。\n",
    "\n",
    "LangChain 提供了多种储存类型：\n",
    "* 缓冲区储存允许保留最近的聊天消息，\n",
    "* 摘要储存则提供了对整个对话的摘要。\n",
    "* 实体储存则允许在多轮对话中保留有关特定实体的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 对话缓存存储\n",
    "\n",
    "缓存存储，也称为对话历史，是 LLM 模型在处理用户输入时，将之前与用户交互的输入和输出进行存储，以供后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好，皮皮鲁！很高兴认识你。我是通义千问，阿里巴巴集团旗下的超大规模语言模型。你可以叫我Qwen。名字很特别啊，皮皮鲁是谁创造的呀？是不是郑渊洁笔下的那个充满想象力和冒险精神的小男孩呢？如果有什么问题或者想聊的话题，尽管告诉我哦！'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试增大该参数。\n",
    "llm = chat #ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "# 新建一个 ConversationChain Class 实例\n",
    "# verbose参数设置为True时，程序会输出更详细的信息，以提供更多的调试或运行时信息。\n",
    "# 相反，当将verbose参数设置为False时，程序会以更简洁的方式运行，只输出关键的信息。\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True )\n",
    "\n",
    "#run 1：\n",
    "conversation.predict(input=\"你好, 我叫皮皮鲁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"1+1等于多少？\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你叫**皮皮鲁**！我刚才已经知道了你的名字，如果有任何其他问题，我都乐意回答哦！ 🚀'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"我叫什么名字？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是通义千问，阿里巴巴集团旗下的超大规模语言模型。你可以叫我Qwen。名字很特别啊，皮皮鲁是谁创造的呀？是不是郑渊洁笔下的那个充满想象力和冒险精神的小男孩呢？如果有什么问题或者想聊的话题，尽管告诉我哦！\n",
      "{'history': 'Human: 你好, 我叫皮皮鲁\\nAI: 你好，皮皮鲁！很高兴认识你。我是通义千问，阿里巴巴集团旗下的超大规模语言模型。你可以叫我Qwen。名字很特别啊，皮皮鲁是谁创造的呀？是不是郑渊洁笔下的那个充满想象力和冒险精神的小男孩呢？如果有什么问题或者想聊的话题，尽管告诉我哦！'}\n",
      "{'history': 'Human: 你好, 我叫皮皮鲁\\nAI: 你好，皮皮鲁！很高兴认识你。我是通义千问，阿里巴巴集团旗下的超大规模语言模型。你可以叫我Qwen。名字很特别啊，皮皮鲁是谁创造的呀？是不是郑渊洁笔下的那个充满想象力和冒险精神的小男孩呢？如果有什么问题或者想聊的话题，尽管告诉我哦！\\nHuman: 很高兴和你成为朋友！\\nAI: 是的，让我们一起去冒险吧！'}\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 对话缓存窗口存储\n",
    "\n",
    "保持缓存窗口存储的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一轮对话：\n",
      "你好，皮皮鲁！很高兴认识你。我是通义千问，一个超爱聊天和分享知识的AI。听说你是个充满想象力和冒险精神的人，是吗？要不要告诉我更多关于你的故事？或者我们可以聊聊有趣的话题，比如科幻、科技、文学……随便你挑！ 😊\n",
      "第二轮对话：\n",
      "1 + 1 等于 **2**。这是最基本的数学运算之一，皮皮鲁！如果你还有其他更复杂或者有趣的数学问题，也可以问我哦！ 😄\n",
      "第三轮对话：\n",
      "你在之前的提问中提到过你的名字是“皮皮鲁”。所以，我叫你皮皮鲁！如果有其他问题，尽管问吧！ 😊\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#k参数控制着窗口大小，即窗口中存储的对话数量。\n",
    "memory2 = ConversationBufferWindowMemory(k=1)\n",
    "conversation2 = ConversationChain(llm=chat, memory=memory2, verbose=False )\n",
    "print(\"第一轮对话：\")\n",
    "print(conversation2.predict(input=\"你好, 我叫皮皮鲁\"))\n",
    "print(\"第二轮对话：\")\n",
    "print(conversation2.predict(input=\"1+1等于多少？\"))\n",
    "print(\"第三轮对话：\")\n",
    "print(conversation2.predict(input=\"我叫什么名字？\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 对话字符缓存存储\n",
    "\n",
    "使用对话字符缓存记忆，内存将限制保存的token数量。如果字符数量超出指定数目，它会切掉这个对话\n",
    "的早期部分以保留与最近的交流相对应的字符数量，但不超过字符限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=30)\n",
    "# memory.save_context({\"input\": \"朝辞白帝彩云间，\"}, {\"output\": \"千里江陵一日还。\"})\n",
    "# memory.save_context({\"input\": \"两岸猿声啼不住，\"}, {\"output\": \"轻舟已过万重山。\"})\n",
    "memory.save_context({\"input\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"}, {\"output\": \"bbbbbbbbbbbbbbbbbbbbbbbbb\"})\n",
    "memory.save_context({\"input\": \"ccccccccccccccccccc\"}, {\"output\": \"ddddddddddddddddddddddddddd\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 对话摘要缓存存储\n",
    "\n",
    "对话摘要缓存储存，使用 LLM 对到目前为止历史对话自动总结摘要，并将其保存下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import ConversationChain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "# 创建一个长字符串\n",
    "schedule = \"在八点你和你的产品团队有一个会议。 \\\n",
    "你需要做一个PPT。 \\\n",
    "上午9点到12点你需要忙于LangChain。\\\n",
    "Langchain是一个有用的工具，因此你的项目进展的非常快。\\\n",
    "中午，在意大利餐厅与一位开车来的顾客共进午餐 \\\n",
    "走了一个多小时的路程与你见面，只为了解最新的 AI。 \\\n",
    "确保你带了笔记本电脑可以展示最新的 LLM 样例.\"\n",
    "# chat = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=100)\n",
    "memory.save_context({\"input\": f\"你好，我叫皮皮鲁\"}, {\"output\": f\"你好啊，我叫鲁西西\"})\n",
    "memory.save_context({\"input\": \"很高兴和你成为朋友！\"}, {\"output\": \"是的，让我们一起去冒险吧！\"})\n",
    "memory.save_context({\"input\": \"今天的日程安排是什么？\"}, {\"output\": f\"{schedule}\"})\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 模型链\n",
    "\n",
    "链（Chains）通常将大语言模型（LLM）与提示（Prompt）结合在一起，基于此，我们可以对文本或数据进行一系列操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 大语言模型链 LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = chat\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"描述制造{product}的一个公司的最佳名称是什么? 直接给出名称\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "product = \"大号床单套装\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 简单顺序链\n",
    "\n",
    "顺序链（SequentialChains）是按预定义顺序执行其链接的链。具体来说，我们将使用简单顺序链（SimpleSequentialChain），这是顺序链的最简单类型，其中每个步骤都有一个输入/输出，一个步骤的输出是下一个步骤的输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m为一家专注于制造大号床单套装的公司起一个好名字，需要结合品牌定位、目标客户群体以及产品的特点。以下是一些可能适合的名称建议：\n",
      "\n",
      "### 1. **KingSize Linens**\n",
      "   - 简洁明了，直接突出“大号”和“床单”的概念。\n",
      "   - “KingSize”传达出高端和宽敞的感觉。\n",
      "\n",
      "### 2. **Grand Beddings**\n",
      "   - “Grand”意味着宏大、豪华，适合用来描述大尺寸的床单套装。\n",
      "   - 给人一种优雅且舒适的品牌印象。\n",
      "\n",
      "### 3. **Majestic Sheets**\n",
      "   - “Majestic”有宏伟、壮丽的意思，能够吸引追求高品质睡眠体验的消费者。\n",
      "   - 强调产品的奢华感和大气设计。\n",
      "\n",
      "### 4. **Extraordinary Beds**\n",
      "   - 名称中的“Extraordinary”（非凡）突出了与众不同，适合用于推广超大尺寸或定制款式的床单套装。\n",
      "\n",
      "### 5. **Spacious Comfort**\n",
      "   - 结合了“空间感”和“舒适度”，非常适合用来形容大号床单带来的宽敞性和柔软触感。\n",
      "\n",
      "### 6. **Royal Dimensions**\n",
      "   - “Royal”象征皇家般的品质，“Dimensions”则暗示产品覆盖各种大尺寸需求。\n",
      "   - 让顾客联想到精致与尊贵。\n",
      "\n",
      "### 7. **Giant Slumber Co.**\n",
      "   - “Giant”代表巨大，而“Slumber”意指睡眠，整体听起来有趣又贴切。\n",
      "   - 适合年轻化、轻松风格的品牌形象。\n",
      "\n",
      "### 8. **Luxury Oversized Linens**\n",
      "   - 如果你的品牌定位于高端市场，这个名称可以很好地传递出奢华和超大尺寸的特点。\n",
      "\n",
      "### 9. **Big Dream Bedding**\n",
      "   - 将“大号”与梦想结合，寓意人们可以在更大的床上享受更美好的梦境。\n",
      "   - 富有情感共鸣，容易记住。\n",
      "\n",
      "### 10. **Monarch Textiles**\n",
      "   - “Monarch”意为君主，象征权威和领导地位。\n",
      "   - 表达出公司在大号床单领域的专业性和领先地位。\n",
      "\n",
      "选择具体名称时，还可以考虑以下几个因素：\n",
      "- 是否容易发音和拼写；\n",
      "- 是否具有独特性，便于商标注册；\n",
      "- 是否能引发积极联想并符合品牌形象。\n",
      "\n",
      "希望这些建议对你有所帮助！\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mKingSize Linens，专注高端大号床单，为追求舒适与品质的顾客带来皇家般睡眠体验。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'KingSize Linens，专注高端大号床单，为追求舒适与品质的顾客带来皇家般睡眠体验。'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# 提示模板 1 ：这个提示将接受产品并返回最佳名称来描述该公司\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "\"描述制造{product}的一个公司的最好的名称是什么\"\n",
    ") \n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "# 提示模板 2 ：接受公司名称，然后输出该公司的长为20个单词的描述\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "\"写一个20字的描述对于下面这个\\\n",
    "公司：{company_name}的\"\n",
    ") \n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "#组装链\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],verbose=True)\n",
    "\n",
    "#执行链\n",
    "product = \"大号床单套装\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 顺序链\n",
    "\n",
    "当只有一个输入和一个输出时，简单顺序链（SimpleSequentialChain）即可实现。当有多个输入或多个输出时，我们则需要使用顺序链（SequentialChain）来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI #导入OpenAI模型\n",
    "from langchain.prompts import ChatPromptTemplate #导入聊天提示模板\n",
    "from langchain.chains import LLMChain #导入LLM链。\n",
    "\n",
    "\n",
    "#子链1\n",
    "# prompt模板 1: 翻译成英语（把下面的review翻译成英语）\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "\"把下面的评论review翻译成英文:\"\n",
    "\"\\n\\n{Review}\"\n",
    ") \n",
    "#chain 1: 输入：Review 输出：英文的 Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
    "#子链2\n",
    "# prompt模板 2: 用一句话总结下面的 review\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "\"请你用一句话来总结下面的评论review:\"\n",
    "\"\\n\\n{English_Review}\"\n",
    ")\n",
    "#chain 2: 输入：英文的Review 输出：总结\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
    "\n",
    "#子链3\n",
    "# prompt模板 3: 下面review使用的什么语言\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "\"下面的评论review使用的什么语言:\\n\\n{Review}\"\n",
    ")\n",
    "#chain 3: 输入：Review 输出：语言\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
    "#子链4\n",
    "# prompt模板 4: 使用特定的语言对下面的总结写一个后续回复\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "\"使用特定的语言对下面的总结写一个后续回复:\"\n",
    "\"\\n\\n总结: {summary}\\n\\n语言: {language}\"\n",
    ") \n",
    "#chain 4: 输入： 总结, 语言 输出： 后续回复\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "output_key=\"followup_message\")\n",
    "\n",
    "\n",
    "\n",
    "#组合四个子链：\n",
    "#输入：review\n",
    "#输出：英文review，总结，后续回复\n",
    "overall_chain = SequentialChain(\n",
    "chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "input_variables=[\"Review\"],\n",
    "output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "verbose=True\n",
    ")\n",
    "\n",
    "#测试运行链\n",
    "review = \"\"\"\\\n",
    " Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre.\n",
    "J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou\n",
    "contrefaçon !?\n",
    "\"\"\"\n",
    "\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 路由链\n",
    "\n",
    "一个路由链，它首先决定将它传递给哪个子链，然后将它传递给那个链。路由器由两个组件组成：\n",
    "* 路由链（Router Chain）：路由器链本身，负责选择要调用的下一个链\n",
    "* destination_chains：路由器链可以路由到的链\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 基于文档的问答\n",
    "\n",
    "使用大语言模型构建一个能够回答关于给定文档和文档集合的问答系统是一种非常实用和有效的应用场\n",
    "景。语言模型不仅利用了自己的通用知识，还可以充分运用外部输入文档的专业信息来回答用户问\n",
    "题，显著提升答案的质量和适用性。构建这类基于外部文档的问答系统，可以让语言模型更好地服务于\n",
    "具体场景，而不是停留在通用层面。\n",
    "\n",
    "基于文档问答的这个过程，我们会涉及 LangChain 中的其他组件，比如：嵌入模型（Embedding\n",
    "Models)和向量储存(Vector Stores)\n",
    "\n",
    "具体应用如RAG(Retrieval Augmented Generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 通过LLM进行评估实例\n",
    "\n",
    "* 首先，我们使用 LLM 自动构建了问答测试集，包含问题及标准答案。\n",
    "* 然后，同一 LLM 试图回答测试集中的所有问题，得到响应。\n",
    "* 下一步，需要评估语言模型的回答是否正确。这里奇妙的是，我们再使用另一个 LLM 链进行判断，所以 LLM 既是“球员”，又是“裁判"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm310",
   "language": "python",
   "name": "llm310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
